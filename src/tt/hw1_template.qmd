---
title: "Homework 1"
subtitle: "Stat 253"
author: "Mohammed Sohail Khan "
date: 14th Sep 2025
format:
  html:
    toc: true
    toc-depth: 2
    embed-resources: true
    fig-width: 5
    fig-height: 3
execute:
  error: true
  warning: false
knitr:
  opts_chunk:
    collapse: true
    message: false
---


# Notes 

(@) Please **update the author name** to your name! 
(@) This document is for your answers only. Do NOT write out the homework questions in this document. Writing out the questions makes it very challenging to grade the homework. (You can create a separate document for your own records if you wish.) 
(@) Besides adding your answers/code, please DO NOT make any deletions or changes to the structure of this template! 



# Setup 

Put any code you need to load packages and load/prep data here. 

```{r}
library(tidyverse)
library(tidymodels)
library(rattle)

data("weatherAUS")
glimpse(weatherAUS)

view(weatherAUS)
```


# Exercises



## Data Context



## Exercise 1



### Part a
filtering 6 rows namely : Hobart, Adelaide, Canberra, Brisbane, Melbourne, and Sydney. 
Mutating(in this case writing over) temp9am values to be farenheit
and then selecting the relevant 6 columns
```{r}
# I used weather_data dataset as I have created a weather dataset in my other class 
weather_data <-weatherAUS %>% 
  filter(Location %in% c("Hobart", "Adelaide", "Canberra", "Brisbane", "Melbourne", "Sydney") ) %>% 
  mutate(Temp9am = Temp9am *1.8 + 32, Temp3pm = Temp3pm * 1.8 + 32) %>% 
  select(Temp3pm, Location, WindSpeed9am, Humidity9am, Pressure9am, 
      Temp9am)
  

```

```{r}
# Checking it out
dim(weather_data)
head(weather_data)
```


### Part b
Arranging it in desending order and then looking at top 3 using head function proving 3 as an argument

```{r}
top_3_hottest <- weather_data %>% 
  arrange(desc(Temp3pm)) %>% 
  head(3)

```

```{r}
# checking it out 
top_3_hottest

```

### Part c
```{r}
temp3pm_6locs <- weather_data %>% 
  group_by(Location) %>% # makes mini groups of every location and then takes mean of the group  based on temp3pm
  summarise(mean(Temp3pm, na.rm = TRUE))
  
```

```{r}
# lets check it out 
temp3pm_6locs

```


### Part d
```{r}
ggplot(weather_data, aes(x = Temp3pm, fill = Location)) + 
  geom_density(alpha = 0.6) # alpha adds translucency to see overlap
  
```
```{r}
# facet_wrap() -- basically splits one plot into smaller sub plots based on similar values in a column 
ggplot(weather_data, aes(x = Temp9am, y = Temp3pm, color = Humidity9am)) + geom_point() + facet_wrap(~Location) +
scale_color_gradient(low = "#132B43", high = "#56B1F7")  

```



### Part e ( re-interpret the b part)
Interpretation : Part-d (a) --- We're comparing 3pm temperature of the 6 cities and its variability(how much its varying). Brisbane's afternoons seems to be in general little hotter relative to other cities as its skewed towards right, higher temp. Similarly Canberra's afternoons seems pretty cool relative to other cities as its skewed towards left. Apart from the extremes, Hobart, Adeliade and Melbourne seems to have overlap and similar temps more skewed towards cold and sydney seems to have most mid-range temperature of them all. 

Interpretation : Part-d (b) --- In this facet's scatter plot, we're comparing humidity of 9 am for different cities. We can see that Adelaide show's linearly decreasing humidity at 9 am when temperatures of 9am and temperatur of 3pm are hotter. So if morning in Adelaide starts warmer and stays warmer till after, then humidity at 9 am,  with certain probablity will be lower. Canberra, Hobart, Melbourne, Canberra and Syndey shows stable higher humidty at 9am, and in all of them at the peak warmer tempertures, humidity seems to drop a bit.


## Exercise 2


### Part a
```{r}
# Specifying the model
lm_spec <- linear_reg() |> 
  set_mode("regression") |> 
  set_engine("lm")

# checking it out
lm_spec
```
```{r}
# fitting the model
weather_model_1 <-lm_spec |> 
  fit(Temp3pm ~ Temp9am + Location + Pressure9am, data = weather_data)

```

```{r}
# checking it out 
weather_model_1 |> 
  tidy()

```

### Part b]
```{r}
# Obtaining the predictions + residuals with augment function as it gives both preidctions and residuals for every row
weather_model_1 %>% 
  augment(new_data = weather_data) %>% 
  head(10)

```
```{r}
# creating the residual plot for weather model 1 
weather_model_1 %>% 
  augment(new_data = weather_data) %>% 
  ggplot(aes(x = .pred, y = .resid)) +
  geom_point() + 
  geom_hline(yintercept = 0) # coz we want to see where our prediction values fall - below or above our redsiual

```
It looks like the model is random, I don't see any particular shape/ pattern like a slope or parabolo etc.This shows that the model is correct.The points seem to be heavily scattered in the middle than tails and there seems to some be outliers but not strong enough to make our model incorrect.



### Part c
```{r}
# getting the metric to evaluate the model's strength
weather_model_1 %>% 
  glance()
```
The weather_model_1 seems strong as indicated by high R^2 of 0.75687. So the predictor variables : temperature at 9am, Location and pressure at 9am explains approximately 76% of variability in our response varaible : temperature at 3pm. Although it's not very strong as roughly 24% of variability is still not explained, but in general it seems strong.  


### Part d

```{r}
# cheking the accuracy -- how far my predictions are off 
weather_model_1 %>% 
  augment(new_data = weather_data) %>% 
  summarise(mae = mean(abs(.resid), na.rm = TRUE)) # had to remove the missing values as it was giving mae as na
```
The mean absolute error is roughly 4, which means that the predictions are off by 4 degrees F for 3pm temperatures,  relative to the scale of the data, which range from 40 degrees F to 120 degress F for 3pm. So being 4 degrees F of in range of  80 degrees F (120-40), is 5% (4/80*100). Therefore, the model is 5% off, which means its statistically 95% accourate. 


### Part e
Evaluating fairness : 
a. This data comes from Australian Commonwealth Bureau of Meteorology which I belive is responsible for providing statitistically efficient predictions and correct meteorological updates to the Austrialian people. In our case, predicting 3pm temperatures, could help people in general stay safe or plan trips. For Businesses, it could them plan their business accordingly, in specific it might help ice cream businesses, tourist businesses, ridesharing businesses etc as they can plan their business according to temperatue. So, It definitely benefits the Australian people.

Since, this data was collected/recorded by taking temperatures, windspeed, pressure change, etc I don't think it involved any animal testing or human testing etc which would have raised ethical concerns, so I think most probably no body was harmed. For our case, predicting 3pm temp, I can't think of anything that could harm anyone. 


### Part f

Temp9am coefficient interpretation : When controlling for location and pressure at 9am, we expect 3pm temperatures to roughly increase by 0.95 degree F for every 1 degree F increase in 9am temperature. 

LocationHobart interpretation : When controlling for temperature at 9am, pressure at 9 am other locations, we expect temperature at 3pm in Hobart to be 1.29 degrees F lower than Adelaide on average. ( Adelaide is our reference variable)

## Exercise 3
```{r}
# Fitting Weather_model_2 with all predictor variables
weather_model_2 <- lm_spec %>% 
   fit(Temp3pm ~ Temp9am + Location + Pressure9am + WindSpeed9am + Humidity9am, data = weather_data)

weather_model_2 %>% 
  tidy()

```
```{r}
# Obtaining prediction and residuals for weather_model_2
weather_model_2 %>% 
  augment(new_data = weather_data) %>% 
  head(10)

```
```{r}
# creating the residual plot for weather_model_2
weather_model_2%>% 
  augment(new_data = weather_data) %>% 
  ggplot(aes(x = .pred, y = .resid)) +
  geom_point() + 
  geom_hline(yintercept = 0)
```
```{r}
# Looking for R^2 for weather_model2 
weather_model_2 %>% 
  glance()
```
```{r}
# checking for accuracy 
weather_model_2 %>% 
  augment(new_data = weather_data) %>% 
  summarise(mae = mean(abs(.resid), na.rm = TRUE))

```

### Part a

```{r}
# Conducting the 10 fold cross validation on model_1 
set.seed(253)
weather_model_1_cv <- lm_spec %>% 
  fit_resamples(Temp3pm ~ Temp9am + Location + Pressure9am, 
  resamples = vfold_cv(weather_data, v = 10),
  metrics = metric_set(mae, rsq)
)

weather_model_1_cv %>% 
  collect_metrics()
  

```


```{r}
set.seed(253)
weather_model_2_cv <- lm_spec %>% 
  fit_resamples(Temp3pm ~ Temp9am + Location + Pressure9am+ Humidity9am+ WindSpeed9am, 
  resamples = vfold_cv(weather_data, v = 10),
  metrics = metric_set(mae, rsq)
)

weather_model_2_cv %>% 
  collect_metrics()
```
 Lets check fold by fold to see if there's any overfitting, just to be sure before comparing.

```{r}
# model 1 fold by fold cv
weather_model_1_cv %>% 
  unnest(.metrics) %>% 
  filter(.metric == "mae")
```

```{r}
weather_model_2_cv %>% 
  unnest(.metrics) %>% 
  filter(.metric == "mae")
```


Okay, so as per the each fold's mae's of both weather_model_1 and  weather_model_2 shows no overfitting as there's no extreme fluctuations which shows that both models are generalizing well to unseen data. Comparatively, based on CV metrics weather_model_2 is better in predicting for 3pm temperatures as it has lower MAE of 4.12 than weather_model_1's 4.24 MAE and higher R^2 of 0.768 than weather_model_1's 0.756.


### Part b

It's better to compare two models using cross validation than in-sample metrics as it can inform us of any overfitting in models based each folds' mae's which could potentially help us to remove the overfitting model as it would not provide better predictions based on new data. 



### Part c

Since creating the folds is random every time we run the code so data is randomly is randomly split, so we set the seed so that data in split is fixed as this helps us with reproducing the same splits everytime so that even if we sharing our code to others or debugging it shows the same results everytime when others run it. 



## Exercise 4

```{r}
melbourne <- read.csv('https://mac-stat.github.io/data/weather_melbourne.csv')
view(melbourne)
```

# I'm just fitting the models as I don't have go back and forth too much
```{r}
# Fitting the model_1 
temp_model_1 <- lm_spec %>% 
  fit(Humidity3pm ~ Temp9am + Humidity9am, data = melbourne)

temp_model_1 %>%
  tidy()
```

```{r}
# Predict the 3pm humidity when at 9am it is 15 degrees (C) with 75% humidity 
temp_model_1 %>% 
  predict(new_data = data.frame(Temp9am = 15, Humidity9am = 75))
```


### Part a
```{r}
temp_model_2 <- lm_spec %>% 
  fit(Humidity3pm ~ Temp9amF + Humidity9am, data = melbourne)
```

1. yes, the temp9amF of temp_model_2 will differ from temp9am of temp_model_2.
2. Yes, p-values will may differ slightly
3. yes, slightly
4. No, I think it will be produce different result




### Part b
```{r}
# Checking if my intuition was correct or not? 
temp_model_1 %>% 
  tidy()

```

```{r}
temp_model_2 %>% 
  tidy()

```

```{r}
temp_model_2 %>% 
  predict(new_data = data.frame(Temp9amF = 15, Humidity9am = 75))

```

- Okay my intuition was incorrect for P values and humidity9am coefficient, as I didnt think they'd be the same, but turned out they are same. And additionally even after scaling different, it didn't seem to change much.


### Part c
No, I think predictor scaling it does not impact overall conclusions as much in linear regression modes. 


### Part d
```{r}
library(mice)
data(mammalsleep)
mammalsleep <- mammalsleep %>% 
  filter(brw > 1) %>% 
  mutate(log_brw = log(brw)) 

```

```{r}
# with brw

# brw density
mammalsleep %>% 
  ggplot(aes(x= brw)) + geom_density()

# gt vs brw
mammalsleep %>% 
  ggplot(aes(x = gt, y = brw)) + geom_point() + 
  geom_smooth(method = "lm", se = FALSE)  # false coz I don't want the shade


# .pred vs .resid ( fitting the model and plotting residual plot)
mammal_gt_brw_model <- lm_spec %>% 
  fit(brw ~ gt, data = mammalsleep) %>% 
  tidy()

mammal_gt_brw_model%>% 
  augment(new_data = mammalsleep) %>% 
  ggplot(aes(x = .pred, y = .resid)) +
  geom_point() + 
  geom_hline(yintercept = 0)



```


```{r}
# with log_brw

# log_brw density
mammalsleep %>% 
  ggplot(aes(x= log_brw)) + geom_density()

# gt vs log_brw
mammalsleep %>% 
  ggplot(aes(x = gt, y = log_brw)) + geom_point() + 
  geom_smooth(method = "lm", se = FALSE)  # false coz I don't want the shade


# .pred vs .resid ( fitting the model and plotting residual plot)
mammal_gt_log_brw_model <- lm_spec %>% 
  fit(log_brw ~ gt, data = mammalsleep)


mammal_gt_log_brw_model%>% 
  augment(new_data = mammalsleep) %>% 
  ggplot(aes(x = .pred, y = .resid)) +
  geom_point() + 
  geom_hline(yintercept = 0)

```
- The log transformations have changed the overall skewness and spread of the brw, as in the density plot of log_brw it shows that less skew and distrubution of the data seems better as its curved and does not show spikes.

- I think the log_brw does fix the model as in the brw residual there's clear heterokedaticity where in the log_brw data points seems more randomly scattered without any pattern or shape. So, I think since log_brw makes resiuals look more random or spread so linearity and the variance of errors basically fit the Linear regression assumptions, therefore log_brw does fix our model.

### Part e

```{r}
# interpreting b1 coefficient of log of log_brw
mammal_gt_log_brw_model %>% 
  tidy()


```
Since the model is now in log scale, instead of brw increasing, it multiplies. So in that case, I think the intepretation for gt of log_brw is : For every additional day of gestation, the expected brain wieght will multiply by 1.0114% (e^0.01127) per day. 

## Exercise 5
Resources I used to complete the HW assignment : 
- Stat 253 Website 
- In class activities
- perplexity to basically ask for r syntax for ggplot 

For me, actually the Stat 253 website was super helpful, and also going through and doing the actvities did help me lot as it made the hw much similar to what we did it class. 

## Exercise 6

- For me changing datasets, and fitting different models was a little challanging as it was hectic but it helped me a ot in getting familiar with the process. 
- I really started feeling better after going through website content, it explained it very well and I just had to apply my reasoning to the models I'm buidling. 
- I started the hw late on Wednesday, beacause I was busy with some other stuff, I did review the material before starting but not throughly but while I was doing thw HW I reviewed it throughly. I did not work with anyone and did not ask questions in OH or Slack.

In the future I think it will be better if I start early and take some help from preceptors. 


# Done!

- Render your QMD to HTML.
- Check that HTML to make sure there are no errors or formatting issues. 
- Submit the HTML on Moodle.


